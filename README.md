# Matthew Levinson
**AI Safety Researcher | Mechanistic Interpretability and Alignment**

I bring deep technical expertise from computational biology, large-scale ML systems, and advanced statistical modeling to mechanistic interpretability research. I am currently focused on understanding how neural networks represent and process information with an eye towards alignment and ensuring the future of humanity.


***Recent Work***<br>
My recent work has explored [information storage versus annotated latent computation](https://openreview.net/pdf?id=svMpLjlExR) in a thinking model's chain of thought, [describing activation distributions](https://www.lesswrong.com/posts/rzpKYr7xYwHgsccLA/beyond-gaussian-language-model-representations-and) in decoder-only transformers, and investigating [classification through activation distributions](https://www.lesswrong.com/posts/6PCjTM55jdYBgHNyp/activation-magnitudes-matter-on-their-own-insights-from-1).

***Current Research***<br>
I am currently working on a new formulation for circuit discovery informed by Bayesian modeling. I am particularly interested in developing this towards unsupervised network decomposition to broadly characterize drivers of critical computation. I am extending my current work in new directions.

***Contact***<br>
[Email Me](mailto:good.epic@gmail.com) | [LinkedIn](https://www.linkedin.com/in/mdlevinson)
